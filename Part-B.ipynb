{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee81e187",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "c7ee8c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "94b9a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_pickle('data.pickle')\n",
    "y = pd.read_pickle('labels.pickle')\n",
    "with open('vocab.txt', 'r') as f:\n",
    "\tvocab = f.read().split(\" \")\n",
    "\tvocab.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "2ad42f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2784, 2784, 9210)"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(y), len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aa34b9",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "51ebb322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "76122f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=777)\n",
    "train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size=0.15, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "854239d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 1, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1]], dtype=int32)"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "8339ef43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1892,), (335,), (557,))"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, val_X.shape, test_X.shape, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "b2f7ca5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1892, 4), (335, 4), (557, 4))"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape, val_y.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "4cc4393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class ClassificationDataset(Dataset):\n",
    "\tdef __init__(self, X, y):\n",
    "\t\tself.X = X\n",
    "\t\tself.y = y\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.X)\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treturn torch.tensor(self.X[idx], dtype=torch.int32), torch.tensor(self.y[idx], dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "c9f0f772",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ClassificationDataset(train_X, train_y)\n",
    "val_dataset = ClassificationDataset(val_X, val_y)\n",
    "test_dataset = ClassificationDataset(test_X, test_y)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a366cfc3",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "0a004bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.functional import F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "b61d2384",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\tdef __init__(self,\n",
    "\t\t\t  vocab_size,\n",
    "\t\t\t  embedding_dim,\n",
    "\t\t\t  hidden_dim,\n",
    "\t\t\t  num_layers,\n",
    "\t\t\t  num_classes,\n",
    "\t\t\t  max_len,\n",
    "\t\t\t  bidirectional,\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# Embeddings, which can be pretrained or normally trained\n",
    "\t\tself.embeddings = nn.Embedding(\n",
    "\t\t\tnum_embeddings=vocab_size,\n",
    "\t\t\tembedding_dim=embedding_dim\n",
    "\t\t)\n",
    "\t\t# LSTM Layer\n",
    "\t\tself.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, bidirectional=bidirectional)\n",
    "\t\t# Linear Layer\n",
    "\t\tif bidirectional:\n",
    "\t\t\tself.linear = nn.Linear(max_len * hidden_dim * 2, num_classes)\n",
    "\t\telse:\n",
    "\t\t\tself.linear = nn.Linear(max_len * hidden_dim, num_classes)\n",
    "\t\tself.softmax = nn.Softmax()\n",
    "\tdef forward(self, x):\n",
    "\t\t# print(x.shape)\n",
    "\t\tembeds = self.embeddings(x)\n",
    "\t\tlstm_out, _ = self.lstm(embeds)\n",
    "\t\tlinear = self.linear(lstm_out.reshape(lstm_out.shape[0], -1))\n",
    "\t\treturn linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd637ba",
   "metadata": {},
   "source": [
    "# Train & Validate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "198d8e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "8dcc2d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(\n",
    "\t\tmodel: LSTM,\n",
    "\t\toptim: Adam,\n",
    "\t\tcriterion: nn.CrossEntropyLoss,\n",
    "\t\tepochs: int,\n",
    "\t\ttrain_dataloader: DataLoader,\n",
    "\t\tval_dataloader: DataLoader,\n",
    "\t\tdevice\n",
    "\t):\n",
    "\tmodel.to(device)\n",
    "\tfor epoch in tqdm(range(epochs)):\n",
    "\t\tmodel.train()\n",
    "\t\ttrain_loss = 0\n",
    "\t\ttrain_correct = 0\n",
    "\t\ttrain_total = 0\n",
    "\t\tfor train_X, train_y in train_dataloader:\n",
    "\t\t\ttrain_X, train_y = train_X.to(device), train_y.to(device)\n",
    "\n",
    "\t\t\ty_preds = model(train_X)\n",
    "\t\t\tloss = criterion(y_preds, train_y)  # FIXED\n",
    "\n",
    "\t\t\toptim.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptim.step()\n",
    "\n",
    "\t\t\ttrain_loss += loss.item()\n",
    "\t\t\tpredicted = torch.argmax(y_preds, dim=1)\n",
    "\t\t\ttrain_correct += (predicted == np.argmax(train_y, axis=1)).sum().item()  # FIXED\n",
    "\t\t\ttrain_total += train_y.size(0)\n",
    "\n",
    "\t\t# Validation\n",
    "\t\tmodel.eval()\n",
    "\t\tval_loss = 0\n",
    "\t\tval_correct = 0\n",
    "\t\tval_total = 0\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor val_X, val_y in val_dataloader:\n",
    "\t\t\t\tval_X, val_y = val_X.to(device), val_y.to(device)\n",
    "\n",
    "\t\t\t\ty_preds = model(val_X)\n",
    "\t\t\t\tloss = criterion(y_preds, val_y)\n",
    "\t\t\t\tval_loss += loss.item()\n",
    "\t\t\t\tpredicted = torch.argmax(y_preds, dim=1)\n",
    "\t\t\t\tval_correct += (predicted == np.argmax(val_y, axis=1)).sum().item()\n",
    "\t\t\t\tval_total += val_y.size(0)\n",
    "\n",
    "\t\tprint(\n",
    "\t\t\tf\"Epoch {epoch+1}/{epochs}, \"\n",
    "\t\t\tf\"Train Loss: {train_loss/len(train_dataloader):.4f}, \"\n",
    "\t\t\tf\"Val Loss: {val_loss/len(val_dataloader):.4f}, \"\n",
    "\t\t\tf\"Train Acc: {100 * train_correct/train_total:.2f}%, \"\n",
    "\t\t\tf\"Val Acc: {100 * val_correct/val_total:.2f}%\"\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "id": "57b4819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n",
    "\n",
    "def grid_search(train_loader, val_loader, criterion, param_grid, vocab_size, num_classes, max_len, device='cpu'):\n",
    "\tresults = []\n",
    "\n",
    "\tkeys = list(param_grid.keys())\n",
    "\tfor values in itertools.product(*param_grid.values()):\n",
    "\t\tparams = dict(zip(keys, values))\n",
    "\t\tprint(f\"Training with params: {params}\")\n",
    "\n",
    "\t\t# Model\n",
    "\t\tmodel = LSTM(\n",
    "\t\t\tvocab_size=vocab_size,\n",
    "\t\t\tembedding_dim=params['embedding_dim'],\n",
    "\t\t\thidden_dim=params['hidden_dim'],\n",
    "\t\t\tnum_layers=params['num_layers'],\n",
    "\t\t\tnum_classes=num_classes,\n",
    "\t\t\tmax_len=max_len,\n",
    "\t\t\tbidirectional=params['bidirectional']\n",
    "\t\t).to(device)\n",
    "\t\toptimizer = Adam(params=model.parameters())\n",
    "\t\t# Training\n",
    "\t\tmodel.train()\n",
    "\t\tfor epoch in range(params['epochs']):\n",
    "\t\t\tfor X_batch, y_batch in train_loader:\n",
    "\t\t\t\tX_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\ty_preds = model(X_batch)\n",
    "\t\t\t\tloss = criterion(y_preds, y_batch)\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\toptimizer.step()\n",
    "\n",
    "\t\t# Validation\n",
    "\t\tmodel.eval()\n",
    "\t\ty_preds_list = []\n",
    "\t\ty_true_list = []\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor X_batch, y_batch in val_loader:\n",
    "\t\t\t\tX_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\t\t\t\ty_preds = model(X_batch)\n",
    "\t\t\t\ty_preds_list.extend(y_preds.numpy())\n",
    "\t\t\t\ty_true_list.extend(torch.argmax(y_batch, dim=1))\n",
    "\n",
    "\t\tf1 = multiclass_f1_score(torch.Tensor(y_preds_list), torch.Tensor(y_true_list), num_classes=4)\n",
    "\t\tprint(f\"Validation accuracy: {f1:.4f}\")\n",
    "\t\tresults.append((params, f1))\n",
    "\n",
    "\treturn sorted(results, key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "ae622d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params: {'embedding_dim': 64, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'lr': 0.001, 'batch_size': 32, 'epochs': 3}\n",
      "Validation accuracy: 0.5642\n",
      "Training with params: {'embedding_dim': 64, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'lr': 0.001, 'batch_size': 32, 'epochs': 3}\n",
      "Validation accuracy: 0.5433\n",
      "Training with params: {'embedding_dim': 64, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'lr': 0.001, 'batch_size': 32, 'epochs': 3}\n",
      "Validation accuracy: 0.5522\n",
      "Training with params: {'embedding_dim': 64, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'lr': 0.001, 'batch_size': 32, 'epochs': 3}\n",
      "Validation accuracy: 0.5433\n",
      "Training with params: {'embedding_dim': 64, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'lr': 0.001, 'batch_size': 32, 'epochs': 3}\n",
      "Validation accuracy: 0.5075\n",
      "Training with params: {'embedding_dim': 64, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'lr': 0.001, 'batch_size': 32, 'epochs': 3}\n",
      "Validation accuracy: 0.5552\n",
      "Training with params: {'embedding_dim': 64, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'lr': 0.001, 'batch_size': 32, 'epochs': 3}\n",
      "Validation accuracy: 0.5582\n",
      "Training with params: {'embedding_dim': 64, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'lr': 0.001, 'batch_size': 32, 'epochs': 3}\n",
      "Validation accuracy: 0.5552\n",
      "Training with params: {'embedding_dim': 128, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': True, 'lr': 0.001, 'batch_size': 32, 'epochs': 3}\n",
      "Validation accuracy: 0.5642\n",
      "Training with params: {'embedding_dim': 128, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'lr': 0.001, 'batch_size': 32, 'epochs': 3}\n",
      "Validation accuracy: 0.5881\n",
      "Training with params: {'embedding_dim': 128, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': True, 'lr': 0.001, 'batch_size': 32, 'epochs': 3}\n",
      "Validation accuracy: 0.5761\n",
      "Training with params: {'embedding_dim': 128, 'hidden_dim': 64, 'num_layers': 2, 'bidirectional': False, 'lr': 0.001, 'batch_size': 32, 'epochs': 3}\n",
      "Validation accuracy: 0.5463\n",
      "Training with params: {'embedding_dim': 128, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': True, 'lr': 0.001, 'batch_size': 32, 'epochs': 3}\n",
      "Validation accuracy: 0.5642\n",
      "Training with params: {'embedding_dim': 128, 'hidden_dim': 128, 'num_layers': 1, 'bidirectional': False, 'lr': 0.001, 'batch_size': 32, 'epochs': 3}\n",
      "Validation accuracy: 0.5701\n",
      "Training with params: {'embedding_dim': 128, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': True, 'lr': 0.001, 'batch_size': 32, 'epochs': 3}\n",
      "Validation accuracy: 0.5821\n",
      "Training with params: {'embedding_dim': 128, 'hidden_dim': 128, 'num_layers': 2, 'bidirectional': False, 'lr': 0.001, 'batch_size': 32, 'epochs': 3}\n",
      "Validation accuracy: 0.5493\n",
      "Best config: ({'embedding_dim': 128, 'hidden_dim': 64, 'num_layers': 1, 'bidirectional': False, 'lr': 0.001, 'batch_size': 32, 'epochs': 3}, tensor(0.5881))\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "\t'embedding_dim': [64, 128],\n",
    "\t'hidden_dim': [64, 128],\n",
    "\t'num_layers': [1, 2],\n",
    "\t'bidirectional': [True, False],\n",
    "\t'lr': [0.001],\n",
    "\t'batch_size': [32],\n",
    "\t'epochs': [3]\n",
    "}\n",
    "\n",
    "best = grid_search(train_dataloader, val_dataloader, nn.CrossEntropyLoss(), param_grid,\n",
    "\t\t\t\t   vocab_size=len(vocab),\n",
    "\t\t\t\t   num_classes=4,\n",
    "\t\t\t\t   max_len=37,\n",
    "\t\t\t\t   device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"Best config:\", best[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6c1dfb",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "ad40d640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "8363a59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "\t\tmodel: LSTM,\n",
    "\t\toptim: Adam,\n",
    "\t\tcriterion: nn.CrossEntropyLoss,\n",
    "\t\ttest_dataloader: DataLoader,\n",
    "\t\tdevice\n",
    "\t):\n",
    "\tmodel.to(device)\n",
    "\tmodel.eval()\n",
    "\ttest_loss = 0\n",
    "\ttest_correct = 0\n",
    "\ttest_total = 0\n",
    "\ty_preds_list = []\n",
    "\twith torch.no_grad():\n",
    "\t\tfor test_X, test_y in test_dataloader:\n",
    "\t\t\ttest_X, test_y = test_X.to(device), test_y.to(device)\n",
    "\n",
    "\t\t\ty_preds = model(test_X)\n",
    "\t\t\ty_preds_list.extend(y_preds.numpy())\n",
    "\t\t\tloss = criterion(test_y, y_preds)\n",
    "\t\t\ttest_loss += loss.item()\n",
    "\t\t\tpredicted = torch.argmax(y_preds, dim=1)\n",
    "\t\t\ttest_correct += (predicted == np.argmax(test_y, axis=1)).sum().item()\n",
    "\t\t\ttest_total += test_y.size(0)\n",
    "\n",
    "\tprint(\n",
    "\t\tf\"test Loss: {test_loss/len(test_dataloader):.4f}, \"\n",
    "\t\tf\"test Acc: {100 * test_correct/test_total:.2f}%\"\n",
    "\t)\n",
    "\treturn y_preds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "e2873f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.1790, test Acc: 56.55%\n"
     ]
    }
   ],
   "source": [
    "y_preds=test(model, optim, criterion, test_dataloader, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "9ca433bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.60      0.59       131\n",
      "           1       0.32      0.32      0.32        87\n",
      "           2       0.00      0.00      0.00        82\n",
      "           3       0.63      0.81      0.71       257\n",
      "\n",
      "    accuracy                           0.57       557\n",
      "   macro avg       0.38      0.43      0.40       557\n",
      "weighted avg       0.47      0.57      0.51       557\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.argmax(test_y, axis=1), np.argmax(y_preds, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b74f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
